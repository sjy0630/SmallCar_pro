import sounddevice as sd
import numpy as np
import requests
import json
import time
import os
import sys
import cv2
import torch
import re
from threading import Thread, Lock
from contextlib import contextmanager
from difflib import SequenceMatcher

# --- å¼•å…¥è¯­éŸ³ä¾èµ– ---
from funasr import AutoModel
from sherpa_onnx import VadModelConfig, SileroVadModelConfig, VoiceActivityDetector

# ==========================================
#               å…¨å±€é…ç½®åŒº
# ==========================================

CAR_IP = "172.24.225.17"
CONTROL_URL = f"http://{CAR_IP}:5000/control"
STREAM_URL = f"http://{CAR_IP}:8080/?action=stream"

API_KEY = "sk-8259e96168f94f4baf816ec8769b726a"
API_URL = "https://api.deepseek.com/chat/completions"
LLM_MODEL_NAME = "deepseek-chat"

VAD_PATH = 'voice_models/VAD/silero_vad.onnx' 
ASR_MODEL_PATH = "iic/SenseVoiceSmall" 

CN_COCO_MAP = {
    "äºº": "person", "æˆ‘": "person", "è‡ªå·±": "person",
    "ç“¶": "bottle", "æ°´": "bottle", "æ¯": "cup",
    "æ‰‹æœº": "cell phone", "ç”µè¯": "cell phone",
    "ä¹¦": "book", "çŒ«": "cat", "ç‹—": "dog",
    "é”®ç›˜": "keyboard", "é¼ æ ‡": "mouse",
    "å‰ªåˆ€": "scissors", "é¥æ§": "remote"
}

CURRENT_MODE = "VOICE"
CURRENT_TARGET = "person"
PROGRAM_RUNNING = True
FORCE_UNLOCK_SIGNAL = False
LAST_CMD_STATE = {"command": "STOP", "steer": 0.0, "throttle": 0.0}

LAST_TRACK_AREA = 0.0
LAST_TRACK_TIME = 0.0

video_lock = Lock()
latest_frame = None
latest_ret = False

session = requests.Session()
session.headers.update({"Authorization": f"Bearer {API_KEY}", "Content-Type": "application/json"})

@contextmanager
def no_print():
    with open(os.devnull, "w") as devnull:
        old_stdout = sys.stdout; old_stderr = sys.stderr
        try:
            sys.stdout = devnull; sys.stderr = devnull
            yield
        finally:
            sys.stdout = old_stdout; sys.stderr = old_stderr

class FuzzySemanticRouter:
    def __init__(self):
        self.corpus = {
            "VOICE_MODE": ["åˆ‡æ¢åˆ°è¯­éŸ³æ¨¡å¼", "æ‰‹åŠ¨æ§åˆ¶", "å¬æˆ‘æŒ‡æŒ¥", "æ”¹ä¸ºè¯­è¨€æ§åˆ¶", "å›æ¥", "åˆ‡å›æ‰‹åŠ¨", "æˆ‘è¦è‡ªå·±å¼€", "äººå·¥æ¨¡å¼", "è§£é™¤è¿½è¸ª", "åœæ­¢è¿½è¸ª", "åˆ«è¿½äº†", "ä¸è¦è·Ÿäº†", "å–æ¶ˆ", "æ¾æ‰‹"],
            "TRACK_MODE": ["å¼€å¯è¿½è¸ª", "åˆ‡æ¢åˆ°è¿½è¸ªæ¨¡å¼", "è·Ÿæˆ‘èµ°", "è‡ªåŠ¨è·Ÿéš", "å¼€å§‹æ‰¾äºº", "è‡ªåŠ¨æ¨¡å¼", "å»è¿½"],
            "FORWARD": ["å‰è¿›", "å¾€å‰èµ°", "å‘å‰", "èµ°èµ·æ¥", "Go", "å¾€å‰", "ç›´è¡Œ", "å¼€"],
            "BACKWARD": ["åé€€", "å€’è½¦", "å¾€åé€€", "é€€å›æ¥", "å‘å"],
            "LEFT": ["å·¦è½¬", "å¾€å·¦æ‹", "å‘å·¦", "è½¬å·¦"],
            "RIGHT": ["å³è½¬", "å¾€å³æ‹", "å‘å³", "è½¬å³"],
            "STOP": ["åœæ­¢", "åœä¸‹", "åˆ«åŠ¨", "åˆ¹è½¦", "ç«‹å®š", "åœ"],
            "SPEED_UP": ["å¤ªæ…¢äº†", "åŠ é€Ÿ", "å¿«ç‚¹è·‘", "æé€Ÿ", "è·‘å¿«ç‚¹", "ç»™æ²¹"],
            "SLOW_DOWN": ["å¤ªå¿«äº†", "å‡é€Ÿ", "æ…¢ä¸€ç‚¹", "æ…¢ç‚¹è·‘", "æ…¢ä¸‹æ¥", "åˆ«é‚£ä¹ˆå¿«"]
        }
        print("ğŸ§  ä»¿ç”Ÿè¯­ä¹‰å¼•æ“å·²å°±ç»ª")

    def predict(self, text, threshold=0.45):
        best_intent = None; best_score = 0.0
        for intent, phrases in self.corpus.items():
            for phrase in phrases:
                score = SequenceMatcher(None, text, phrase).ratio()
                if phrase in text: score = 1.0 
                if score > best_score: best_score = score; best_intent = intent
        return best_intent if best_score >= threshold else None

router = FuzzySemanticRouter()

# ==========================================
#           æ¨¡å— 1: è§†è§‰è¿½è¸ª (v15 æé€Ÿæµä¼ ç‰ˆ)
# ==========================================
Kp = 0.35; Ki = 0.0; Kd = 1.0; CENTER_DEAD_ZONE = 0.12
STOP_AREA_THRESHOLD = 0.30 
MAX_LOCK_AREA = 0.45 
CONF_THRESHOLD = 0.35
ERROR_BUFFER_SIZE = 3

integral = 0.0; previous_error = 0.0; last_pid_time = time.time(); error_buffer = []

def load_yolo_model():
    print("ğŸ“· æ­£åœ¨åŠ è½½æœ¬åœ° YOLOv5s æ¨¡å‹ (ç¦»çº¿æ¨¡å¼)...")
    try:
        model = torch.hub.load('./yolov5', 'custom', path='yolov5s.pt', source='local')
        model.conf = CONF_THRESHOLD; model.iou = 0.45
        if torch.cuda.is_available(): model.cuda()
        print("âœ… YOLOv5s åŠ è½½æˆåŠŸï¼")
        return model
    except Exception as e:
        print(f"âŒ è§†è§‰æ¨¡å‹åŠ è½½å¤±è´¥: {e}"); return None

def get_pid_command(box, h, w):
    global integral, previous_error, last_pid_time, error_buffer
    x_center = (box[0] + box[2]) / 2 / w
    curr_time = time.time(); dt = curr_time - last_pid_time; 
    if dt == 0: dt = 1e-5
    error_buffer.append(x_center - 0.5)
    if len(error_buffer) > ERROR_BUFFER_SIZE: error_buffer.pop(0)
    smooth_error = sum(error_buffer) / len(error_buffer)
    if abs(smooth_error) < CENTER_DEAD_ZONE: smooth_error = 0.0; previous_error = 0.0
    integral += smooth_error * dt; integral = max(-1.0, min(1.0, integral))
    derivative = (smooth_error - previous_error) / dt
    active_Kp = 0.55 if CURRENT_TARGET != "person" else 0.35
    steer = (active_Kp * smooth_error) + (Ki * integral) + (Kd * derivative)
    previous_error = smooth_error; last_pid_time = curr_time
    return "FORWARD", max(-1.0, min(1.0, steer))

def video_loop():
    global latest_frame, latest_ret, CURRENT_MODE, PROGRAM_RUNNING, CURRENT_TARGET, FORCE_UNLOCK_SIGNAL, LAST_TRACK_AREA, LAST_TRACK_TIME
    yolo = load_yolo_model()
    if not yolo: return
    cap = cv2.VideoCapture(STREAM_URL)
    
    def read_stream():
        global latest_frame, latest_ret
        while PROGRAM_RUNNING:
            try:
                ret, frame = cap.read()
                # [æé€Ÿæ ¸å¿ƒ 1] å¼ºåˆ¶é™ä½åˆ†è¾¨ç‡åˆ° 640x480
                # å¦‚æœä½ ä¸åŠ è¿™è¡Œï¼Œä¼ è¿‡æ¥ 1080P ä¼šå¡æ­»
                if ret: frame = cv2.resize(frame, (640, 480))
                with video_lock: latest_frame = frame; latest_ret = ret
                if not ret: time.sleep(0.5)
                # æçŸ­çš„ sleep è®©å‡º CPU
                else: time.sleep(0.005) 
            except: pass
            
    Thread(target=read_stream, daemon=True).start()
    tracker = None; is_tracking = False; frames_since = 0; loop_counter = 0
    print("âœ… è§†è§‰çº¿ç¨‹å¯åŠ¨ (v15 æé€Ÿç‰ˆ)")
    
    while PROGRAM_RUNNING:
        try:
            with video_lock: frame = latest_frame.copy() if latest_frame is not None else None
            if frame is None: time.sleep(0.05); continue
            h, w, _ = frame.shape
            
            if FORCE_UNLOCK_SIGNAL: 
                is_tracking = False; tracker = None; FORCE_UNLOCK_SIGNAL = False; LAST_TRACK_AREA = 0.0
                send_command("STOP", 0, 0); print("ğŸ”“ è§†è§‰å·²é‡ç½®")

            if CURRENT_MODE == "TRACK":
                need_detect = (not is_tracking) or (frames_since > 10)
                cands = []
                
                # [æé€Ÿæ ¸å¿ƒ 2] è·³å¸§æ£€æµ‹ï¼šå¦‚æœæ˜¯æœç´¢æ¨¡å¼ï¼Œæ¯ 3 å¸§æ‰è·‘ä¸€æ¬¡ YOLOï¼Œä¸­é—´çš„å¸§åªæ˜¾ç¤ºç”»é¢
                # è¿™èƒ½å¤§å¹…æé«˜æ˜¾ç¤ºçš„æµç•…åº¦ï¼Œå‡å°‘å¡é¡¿
                should_run_yolo = True
                if not is_tracking:
                    loop_counter += 1
                    if loop_counter % 3 != 0: should_run_yolo = False

                if need_detect and should_run_yolo:
                    frames_since = 0
                    try:
                        results = yolo(frame)
                        for det in results.xyxy[0].cpu().numpy():
                            x1, y1, x2, y2, conf, cls = det
                            if int(cls) < len(results.names):
                                name = results.names[int(cls)]
                                if name == CURRENT_TARGET:
                                    cv2.rectangle(frame, (int(x1),int(y1)), (int(x2),int(y2)), (255,0,0), 1)
                                    b_w = x2-x1; b_h = y2-y1; ratio = (b_w*b_h)/(w*h)
                                    if ratio < 0.6: cands.append([x1,y1,x2,y2])
                    except: pass

                if not is_tracking:
                    if cands:
                        best = min(cands, key=lambda b: ((b[0]+b[2])/2/w-0.5)**2 + ((b[1]+b[3])/2/h-0.5)**2)
                        
                        # [æé€Ÿæ ¸å¿ƒ 3] å¦‚æœ CSRT å¤ªå¡ï¼Œå¯ä»¥æ”¹ç”¨ KCF (é€Ÿåº¦æå¿«ä½†ä¸å¦‚CSRTå‡†)
                        # tracker = cv2.TrackerKCF_create() 
                        tracker = cv2.TrackerCSRT_create() # é»˜è®¤ä¸ºå‡†åº¦ä¼˜å…ˆ
                        
                        tracker.init(frame, (int(best[0]), int(best[1]), int(best[2]-best[0]), int(best[3]-best[1])))
                        is_tracking = True; print(f"ğŸ¯ é”å®šç›®æ ‡: {CURRENT_TARGET}")
                    else:
                        if should_run_yolo: # åªæœ‰è·‘äº† YOLO ä¸”æ²¡æ‰¾åˆ°ï¼Œæ‰åˆ¤æ–­ç›²åŒº
                            time_diff = time.time() - LAST_TRACK_TIME
                            if LAST_TRACK_AREA > 0.15 and time_diff < 1.0:
                                print(f"ğŸ›‘ è§†é‡é®æŒ¡ï¼Œç›²åŒºåœè½¦"); send_command("STOP", 0, 0)
                                cv2.putText(frame, "BLIND SPOT", (20, 150), 0, 1.0, (0,0,255), 3)
                            else:
                                cv2.putText(frame, "SEARCHING...", (20, 80), 0, 0.7, (0,165,255), 2)
                                send_command("FORWARD", 1.0, 0.35) 
                else:
                    # è¿½è¸ªæ—¶æ¯ä¸€å¸§éƒ½è¦è·‘ï¼Œå¦åˆ™è·Ÿä¸ä½
                    frames_since += 1
                    ok, bbox = tracker.update(frame)
                    if ok:
                        box_area = bbox[2] * bbox[3] / (w * h)
                        LAST_TRACK_AREA = box_area; LAST_TRACK_TIME = time.time()
                        
                        y_bottom = bbox[1] + bbox[3]
                        cond_area = box_area > STOP_AREA_THRESHOLD
                        cond_bottom = y_bottom > (h * 0.95)
                        cond_full = (bbox[1] < h * 0.05) and (y_bottom > h * 0.90)

                        if cond_area or cond_bottom or cond_full:
                            print(f"ğŸ›‘ é˜²æ’è§¦å‘"); send_command("STOP", 0, 0)
                            is_tracking = False; tracker = None; LAST_TRACK_AREA = 0.0
                            time.sleep(1.0); continue

                        p1 = (int(bbox[0]), int(bbox[1])); p2 = (int(bbox[0]+bbox[2]), int(bbox[1]+bbox[3]))
                        cv2.rectangle(frame, p1, p2, (0,255,0), 3)
                        cv2.putText(frame, f"LOCKED {box_area:.2f}", (p1[0], p1[1]-10), 0, 0.7, (0,255,0), 2)
                        cmd, steer = get_pid_command([bbox[0], bbox[1], bbox[0]+bbox[2], bbox[1]+bbox[3]], h, w)
                        send_command(cmd, steer, 0.35 if cmd=="FORWARD" else 0.0)
                    else:
                        is_tracking = False; tracker = None; send_command("STOP", 0, 0)
            else:
                if is_tracking: is_tracking=False; tracker=None
                cv2.putText(frame, "VOICE MODE", (20, 40), 0, 1.0, (0,0,255), 2)
            cv2.imshow('SmartCar Vision v15 (Fast)', frame)
            if cv2.waitKey(1) == ord('q'): PROGRAM_RUNNING=False; break
        except Exception as e: time.sleep(0.01)
    cap.release(); cv2.destroyAllWindows()

# ==========================================
#           æ¨¡å— 2: è¯­éŸ³æ§åˆ¶
# ==========================================
SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½å°è½¦çš„æ§åˆ¶å¤§è„‘ã€‚è¯·å°†ç”¨æˆ·çš„å£è¯­æŒ‡ä»¤è½¬æ¢ä¸º JSON æ§åˆ¶ä¿¡å·ã€‚
{ "command": "FORWARD/BACKWARD/STOP", "steer": -1.0åˆ°1.0, "throttle": 0.0åˆ°1.0, "target": "person/cup/bottle/..." }
"""

def send_command(cmd, steer, throttle):
    global LAST_CMD_STATE
    LAST_CMD_STATE = {"command": cmd, "steer": steer, "throttle": throttle}
    try: requests.post(CONTROL_URL, json=LAST_CMD_STATE, timeout=0.2)
    except: pass 

def handle_ai_command(text):
    cmd_data = {"mode_switch": None, "command": "STOP", "steer": 0.0, "throttle": 0.0, "new_target": None, "unlock": False}
    intent = router.predict(text)
    
    if intent:
        print(f"ğŸ§  è¯­ä¹‰ç†è§£: [{intent}]")
        if intent == "VOICE_MODE": cmd_data["mode_switch"] = "VOICE"; return cmd_data
        if intent == "TRACK_MODE": cmd_data["mode_switch"] = "TRACK"; return cmd_data
        
        current_steer = 0.0; current_throttle = 0.35; current_cmd = "FORWARD"
        if intent in ["SPEED_UP", "SLOW_DOWN"]:
            current_cmd = LAST_CMD_STATE.get("command", "FORWARD")
            if current_cmd == "STOP": current_cmd = "FORWARD"
            current_steer = LAST_CMD_STATE.get("steer", 0.0)
        
        if intent == "FORWARD": current_cmd = "FORWARD"
        elif intent == "BACKWARD": current_cmd = "BACKWARD"
        elif intent == "STOP": current_cmd = "STOP"
        if intent == "LEFT": current_steer = -1.0; current_cmd = "FORWARD"
        elif intent == "RIGHT": current_steer = 1.0; current_cmd = "FORWARD"
        if intent == "SPEED_UP": current_throttle = 0.6
        elif intent == "SLOW_DOWN": current_throttle = 0.2
            
        cmd_data["command"] = current_cmd; cmd_data["steer"] = current_steer; cmd_data["throttle"] = current_throttle
        return cmd_data
    
    tgt = next((v for k,v in CN_COCO_MAP.items() if k in text), None)
    if tgt and (len(text) < 8 or "è¿½" in text or "è·Ÿ" in text or "æ‰¾" in text):
        cmd_data["mode_switch"]="TRACK"; cmd_data["new_target"]=tgt; return cmd_data

    print("ğŸ¤” æœ¬åœ°è¯­ä¹‰ä¸ç¡®å®šï¼Œè¯·æ±‚ LLM æ”¯æ´...")
    return None 

def audio_loop():
    global CURRENT_MODE, PROGRAM_RUNNING, CURRENT_TARGET, FORCE_UNLOCK_SIGNAL
    print("ğŸ™ï¸ åˆå§‹åŒ–è¯­éŸ³ (v15)...")
    try:
        asr = AutoModel(model=ASR_MODEL_PATH, disable_update=True, log_level="ERROR")
        vad = VoiceActivityDetector(VadModelConfig(SileroVadModelConfig(model=VAD_PATH, min_silence_duration=0.5, threshold=0.5), sample_rate=16000), buffer_size_in_seconds=100)
        print("âœ… è¯­éŸ³å°±ç»ª")
    except Exception as e: print(f"âŒ è¯­éŸ³æŒ‚äº†: {e}"); return

    sr = 16000; batch = int(0.1*sr)
    with sd.InputStream(channels=1, dtype="float32", samplerate=sr) as s:
        while PROGRAM_RUNNING:
            d, _ = s.read(batch); d = d.reshape(-1)
            vad.accept_waveform(d)
            if not vad.empty():
                raw = np.array(vad.front.samples); vad.pop()
                if len(raw) > 0:
                    try:
                        with no_print(): 
                            res = asr.generate(input=[raw], cache={}, language="zh", use_itn=True, batch_size_s=60)
                        
                        text = res[0].get("text", "") if res else ""; text = re.sub(r'<\|.*?\|>', '', text).strip()
                        if text:
                            print(f"\nğŸ‘‚: {text}")
                            data = handle_ai_command(text)
                            if not data and len(text) > 1: 
                                payload = {"model": LLM_MODEL_NAME, "messages": [{"role": "system", "content": SYSTEM_PROMPT}, {"role": "user", "content": text}], "stream": False, "response_format": {"type": "json_object"}}
                                try: 
                                    print("ğŸ¤– æ­£åœ¨æ€è€ƒ...")
                                    r = session.post(API_URL, json=payload, timeout=3).json()['choices'][0]['message']['content']
                                    data = json.loads(r.replace("```json","").replace("```",""))
                                except: pass

                            if not data: continue
                            if data.get("unlock"): FORCE_UNLOCK_SIGNAL=True; print("ğŸ”“ æŒ‡ä»¤: è§£é™¤è¿½è¸ª"); continue
                            if data.get("new_target"): CURRENT_TARGET=data["new_target"]; FORCE_UNLOCK_SIGNAL=True; print(f"ğŸ¯ æ–°ç›®æ ‡: {CURRENT_TARGET}")
                            if data.get("mode_switch"): 
                                if data["mode_switch"] != CURRENT_MODE: CURRENT_MODE=data["mode_switch"]; print(f"ğŸ”€ æ¨¡å¼: {CURRENT_MODE}"); send_command("STOP",0,0); continue
                            
                            if CURRENT_MODE=="VOICE": 
                                send_command(data.get("command","STOP"), data.get("steer",0), data.get("throttle",0))
                    except Exception as e: 
                        if "WinError 6" not in str(e): print(f"è¯­éŸ³é”™è¯¯: {e}")

if __name__ == "__main__":
    t = Thread(target=video_loop, daemon=True); t.start()
    try:
        audio_loop()
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·å¼ºåˆ¶ä¸­æ–­...")
    finally:
        PROGRAM_RUNNING = False
        print("ğŸ›‘ æ­£åœ¨æ‰§è¡Œç´§æ€¥åœè½¦...")
        for _ in range(3):
            send_command("STOP", 0.0, 0.0)
            time.sleep(0.1)
        print("âœ… å°è½¦å·²å®‰å…¨åœæ­¢ã€‚")