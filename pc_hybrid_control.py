import sounddevice as sd
import numpy as np
import requests
import json
import time
import os
import sys
import cv2
import torch
import re
from threading import Thread, Lock

# --- å¼•å…¥ä¾èµ– ---
from funasr import AutoModel
from sherpa_onnx import VadModelConfig, SileroVadModelConfig, VoiceActivityDetector
import model.detector
import utils.utils

# ==========================================
#               å…¨å±€é…ç½®åŒº
# ==========================================

CAR_IP = "172.24.225.17"
CONTROL_URL = f"http://{CAR_IP}:5000/control"
STREAM_URL = f"http://{CAR_IP}:8080/?action=stream"

API_KEY = "sk-8259e96168f94f4baf816ec8769b726a"
API_URL = "https://api.deepseek.com/chat/completions"
LLM_MODEL_NAME = "deepseek-chat"

VAD_PATH = 'voice_models/VAD/silero_vad.onnx' 
ASR_MODEL_PATH = "iic/SenseVoiceSmall" 
YOLO_CFG_DATA = 'data/coco.data'
YOLO_WEIGHTS = 'modelzoo/coco2017-0.241078ap-model.pth'
YOLO_NAMES = 'data/coco.names'

# æ˜ å°„è¡¨
CN_COCO_MAP = {
    "äºº": "person", "æˆ‘": "person", "è‡ªå·±": "person",
    "ç“¶": "bottle", "æ°´": "bottle",
    "æ¯": "cup",
    "æ‰‹æœº": "cell phone", "ç”µè¯": "cell phone",
    "ä¹¦": "book",
    "çŒ«": "cat", "ç‹—": "dog",
    "çƒ": "sports ball",
    "è½¦": "car", "æ¤…": "chair", "é”®ç›˜": "keyboard", "é¼ æ ‡": "mouse"
}

# å…¨å±€çŠ¶æ€
CURRENT_MODE = "VOICE"
CURRENT_TARGET = "person"
PROGRAM_RUNNING = True
# æ–°å¢ï¼šå¼ºåˆ¶è§£é”ä¿¡å·
FORCE_UNLOCK_SIGNAL = False

video_lock = Lock()
latest_frame = None
latest_ret = False

session = requests.Session()
session.headers.update({"Authorization": f"Bearer {API_KEY}", "Content-Type": "application/json"})

# ==========================================
#           æ¨¡å— 1: è§†è§‰è¿½è¸ª (v3 å¢å¼ºç‰ˆ)
# ==========================================

Kp = 0.35; Ki = 0.0; Kd = 1.0
CENTER_DEAD_ZONE = 0.12
STOP_AREA_THRESHOLD = 0.3 # ç¨å¾®æ”¾å®½åœæ­¢è·ç¦»
MAX_LOCK_AREA = 0.6       # æ”¾å®½æœ€å¤§é¢ç§¯ï¼Œé˜²æ­¢è·Ÿä¸¢è¿‘å¤„çš„ç‰©ä½“
# [å…³é”®ä¿®æ”¹] å¤§å¹…é™ä½ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œè®©å®ƒèƒ½çœ‹åˆ°æ›´å¤šä¸œè¥¿
CONF_THRESHOLD = 0.20     
IOU_THRESHOLD = 0.4
ERROR_BUFFER_SIZE = 3

integral = 0.0
previous_error = 0.0
last_pid_time = time.time()
error_buffer = []

def load_yolo_model():
    print("ğŸ“· åŠ è½½ YOLO æ¨¡å‹...")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    cfg = utils.utils.load_datafile(YOLO_CFG_DATA)
    yolo_net = model.detector.Detector(cfg["classes"], cfg["anchor_num"], True).to(device)
    yolo_net.load_state_dict(torch.load(YOLO_WEIGHTS, map_location=device))
    yolo_net.eval()
    
    label_names = ["person"]
    if os.path.exists(YOLO_NAMES):
        with open(YOLO_NAMES, 'r', encoding='utf-8') as f:
            label_names = [line.strip() for line in f.readlines()]
    return yolo_net, device, cfg, label_names

def get_pid_command(box, h, w):
    global integral, previous_error, last_pid_time, error_buffer
    
    x_center = (box[0] + box[2]) / 2 / w
    box_area = (box[2] - box[0]) * (box[3] - box[1]) / (h * w)
    
    if box_area > STOP_AREA_THRESHOLD:
        integral = 0.0; previous_error = 0.0; error_buffer = []
        return "STOP", 0.0
    
    curr_time = time.time()
    dt = curr_time - last_pid_time
    if dt == 0: dt = 1e-5
    
    target_x = 0.5
    raw_error = x_center - target_x
    
    error_buffer.append(raw_error)
    if len(error_buffer) > ERROR_BUFFER_SIZE: error_buffer.pop(0)
    smooth_error = sum(error_buffer) / len(error_buffer)
    
    if abs(smooth_error) < CENTER_DEAD_ZONE:
        smooth_error = 0.0; previous_error = 0.0
        
    integral += smooth_error * dt
    integral = max(-1.0, min(1.0, integral))
    derivative = (smooth_error - previous_error) / dt
    
    # é’ˆå¯¹å°ç‰©ä½“å¢åŠ çµæ•åº¦
    active_Kp = 0.55 if CURRENT_TARGET != "person" else 0.35
    steer = (active_Kp * smooth_error) + (Ki * integral) + (Kd * derivative)
    previous_error = smooth_error
    last_pid_time = curr_time
    
    return "FORWARD", max(-1.0, min(1.0, steer))

def video_loop():
    global latest_frame, latest_ret, CURRENT_MODE, PROGRAM_RUNNING, CURRENT_TARGET, FORCE_UNLOCK_SIGNAL
    
    yolo_net, device, cfg, label_names = load_yolo_model()
    
    cap = cv2.VideoCapture(STREAM_URL)
    
    def read_stream():
        global latest_frame, latest_ret
        while PROGRAM_RUNNING:
            ret, frame = cap.read()
            with video_lock:
                latest_frame = frame; latest_ret = ret
            if not ret: time.sleep(0.5)
            else: time.sleep(0.01)
    
    t_read = Thread(target=read_stream, daemon=True)
    t_read.start()
    
    tracker = None
    is_tracking = False
    frames_since_detect = 0
    RE_DETECT_INTERVAL = 10 # åŠ å¿«é‡æ£€æµ‹é¢‘ç‡
    
    print("âœ… è§†è§‰çº¿ç¨‹å¯åŠ¨")
    
    while PROGRAM_RUNNING:
        with video_lock:
            frame = latest_frame.copy() if latest_frame is not None else None
            ret = latest_ret
            
        if not ret or frame is None:
            time.sleep(0.1); continue
            
        h, w, _ = frame.shape
        scale_h, scale_w = h / cfg["height"], w / cfg["width"]
        
        # --- å¤„ç†å¼ºåˆ¶è§£é”ä¿¡å· ---
        if FORCE_UNLOCK_SIGNAL:
            print("ğŸ”“ è§†è§‰æ”¶åˆ°æŒ‡ä»¤: è§£é™¤é”å®š")
            is_tracking = False
            tracker = None
            FORCE_UNLOCK_SIGNAL = False # å¤ä½ä¿¡å·
            send_command("STOP", 0.0, 0.0)

        # ä»…åœ¨ TRACK æ¨¡å¼å·¥ä½œ
        if CURRENT_MODE == "TRACK":
            need_detection = (not is_tracking) or (frames_since_detect > RE_DETECT_INTERVAL)
            yolo_boxes = []
            
            if need_detection:
                frames_since_detect = 0
                res_img = cv2.resize(frame, (cfg["width"], cfg["height"]), interpolation=cv2.INTER_LINEAR)
                img = res_img.reshape(1, cfg["height"], cfg["width"], 3)
                img = torch.from_numpy(img.transpose(0, 3, 1, 2)).to(device).float() / 255.0
                
                preds = yolo_net(img)
                output = utils.utils.handel_preds(preds, cfg, device)
                output_boxes = utils.utils.non_max_suppression(output, conf_thres=CONF_THRESHOLD, iou_thres=IOU_THRESHOLD)
                
                for box in output_boxes[0]:
                    box_list = box.tolist()
                    cls_id = int(box_list[5])
                    if cls_id < len(label_names):
                        cat = label_names[cls_id].strip()
                        
                        # [è°ƒè¯•åŠŸèƒ½] ç»˜åˆ¶æ‰€æœ‰è¯†åˆ«åˆ°çš„å€™é€‰æ¡†(è“è‰²)ï¼Œæ–¹ä¾¿çœ‹æ¨¡å‹æœ‰æ²¡æœ‰"ç"
                        if cat == CURRENT_TARGET:
                            bx1 = int(box_list[0]*scale_w); by1 = int(box_list[1]*scale_h)
                            bx2 = int(box_list[2]*scale_w); by2 = int(box_list[3]*scale_h)
                            # è“è‰²ç»†æ¡†è¡¨ç¤ºï¼šYOLO çœ‹åˆ°äº†è¿™ä¸ªï¼Œä½†æ˜¯è¿˜æ²¡é”å®š
                            cv2.rectangle(frame, (bx1, by1), (bx2, by2), (255, 0, 0), 1)
                            cv2.putText(frame, f"{cat}", (bx1, by1-5), 0, 0.5, (255,0,0), 1)

                            b_w = box_list[2] - box_list[0]
                            b_h = box_list[3] - box_list[1]
                            area_ratio = (b_w * b_h) / (cfg["width"] * cfg["height"])
                            
                            # --- å·®å¼‚åŒ–è¿‡æ»¤é€»è¾‘ ---
                            if CURRENT_TARGET == "person":
                                # äººï¼šä¸¥æ ¼è¿‡æ»¤ï¼Œé˜²æ­¢æŠŠå¢™å½“äºº
                                aspect_ok = b_h > (b_w * 0.8) # å¿…é¡»æ˜¯ç˜¦é«˜çš„
                                if area_ratio < MAX_LOCK_AREA and aspect_ok:
                                    yolo_boxes.append(box_list)
                            else:
                                # ç‰©ä½“ï¼šæåº¦å®½å®¹ï¼
                                # åªè¦ä¸æ˜¯å¤§å¾—ç¦»è°±(æ¯”å¦‚è¯¯æ£€äº†æ•´ä¸ªåœ°æ¿)ï¼Œéƒ½è¦
                                if area_ratio < 0.9: 
                                    yolo_boxes.append(box_list)
            
            # è¿½è¸ªé€»è¾‘
            if not is_tracking:
                if len(yolo_boxes) > 0:
                    # æ‰¾ç¦»ä¸­å¿ƒæœ€è¿‘çš„
                    def dist_center(b):
                        cx = (b[0]+b[2])/2/cfg["width"]; cy = (b[1]+b[3])/2/cfg["height"]
                        return (cx-0.5)**2 + (cy-0.5)**2
                    best_box = min(yolo_boxes, key=dist_center)
                    
                    print(f"ğŸ¯ [è§†è§‰] é”å®šç›®æ ‡: {CURRENT_TARGET}")
                    tracker = cv2.TrackerCSRT_create()
                    x1 = int(best_box[0]*scale_w); y1 = int(best_box[1]*scale_h)
                    wb = int((best_box[2]-best_box[0])*scale_w); hb = int((best_box[3]-best_box[1])*scale_h)
                    tracker.init(frame, (x1, y1, wb, hb))
                    is_tracking = True
                else:
                    # æ²¡æ‰¾åˆ°ç›®æ ‡æ—¶ï¼Œæ˜¾ç¤ºæ­£åœ¨å¯»æ‰¾
                    cv2.putText(frame, f"SEARCHING: {CURRENT_TARGET}...", (20, 80), 0, 0.7, (0, 255, 255), 2)
            else:
                frames_since_detect += 1
                ok, bbox = tracker.update(frame)
                if ok:
                    p1 = (int(bbox[0]), int(bbox[1]))
                    p2 = (int(bbox[0]+bbox[2]), int(bbox[1]+bbox[3]))
                    # ç»¿è‰²ç²—æ¡†ï¼šè¡¨ç¤ºæ­£åœ¨è¿½è¸ª
                    cv2.rectangle(frame, p1, p2, (0, 255, 0), 3)
                    cv2.putText(frame, f"LOCKED: {CURRENT_TARGET}", (p1[0], p1[1]-10), 0, 0.7, (0,255,0), 2)
                    
                    box_for_pid = [bbox[0]/scale_w, bbox[1]/scale_h, (bbox[0]+bbox[2])/scale_w, (bbox[1]+bbox[3])/scale_h]
                    move_cmd, steer_cmd = get_pid_command(box_for_pid, cfg["height"], cfg["width"])
                    send_command(move_cmd, steer_cmd, 0.35 if move_cmd == "FORWARD" else 0.0)
                else:
                    is_tracking = False
                    tracker = None
                    send_command("STOP", 0.0, 0.0)
        else:
            if is_tracking: is_tracking = False; tracker = None
            cv2.putText(frame, "VOICE MODE", (20, 40), 0, 1.0, (0, 0, 255), 2)

        cv2.imshow('SmartCar Vision (Blue=Candidate, Green=Locked)', frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            PROGRAM_RUNNING = False
            break
            
    cap.release()
    cv2.destroyAllWindows()

# ==========================================
#           æ¨¡å— 2: è¯­éŸ³æ§åˆ¶ (v3 å¢å¼ºç‰ˆ)
# ==========================================

SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½å°è½¦ã€‚ç”¨æˆ·è¯´ä¸­æ–‡ï¼Œä½ è¾“å‡ºJSONã€‚
{ "command": "FORWARD/BACKWARD/STOP", "steer": -1.0åˆ°1.0, "throttle": 0.0åˆ°1.0, "target": "person/cup/bottle/..." }
"""

def send_command(cmd, steer, throttle):
    try:
        data = {"command": cmd, "steer": steer, "throttle": throttle}
        requests.post(CONTROL_URL, json=data, timeout=0.2)
    except: pass 

def parse_local_logic(text: str):
    """
    æé€Ÿæœ¬åœ°è§£æ
    """
    cmd = {"mode_switch": None, "command": "STOP", "steer": 0.0, "throttle": 0.0, "new_target": None, "unlock": False}
    
    # 0. ä¼˜å…ˆå¤„ç†ï¼šå¼ºåˆ¶è§£é”/å–æ¶ˆ
    if any(w in text for w in ["è§£é”", "å–æ¶ˆ", "åˆ«è¿½", "æ¾æ‰‹", "æ”¾å¼€"]):
        cmd["unlock"] = True
        return cmd

    # 1. åˆ‡æ¢è¿½è¸ªç›®æ ‡
    detected_obj = None
    for cn_key, en_val in CN_COCO_MAP.items():
        if cn_key in text:
            detected_obj = en_val
            break 
            
    if ("è¿½" in text or "è·Ÿ" in text or "æ‰¾" in text) and detected_obj:
        cmd["mode_switch"] = "TRACK"
        cmd["new_target"] = detected_obj
        return cmd 

    if "è¿½è¸ª" in text or "è‡ªåŠ¨" in text or "è·Ÿæˆ‘èµ°" in text:
        cmd["mode_switch"] = "TRACK"
        return cmd

    # 2. å›æ‰‹åŠ¨æ¨¡å¼
    if any(w in text for w in ["æ‰‹åŠ¨", "å¬æˆ‘", "åœæ­¢è¿½è¸ª"]):
        cmd["mode_switch"] = "VOICE"
        return cmd

    # 3. è¿åŠ¨æŒ‡ä»¤
    is_motion = any(w in text for w in ["å‰", "å", "å·¦", "å³", "åœ", "å¿«", "æ…¢", "é€€", "èµ°"])
    if is_motion:
        cmd["command"] = "FORWARD"
        cmd["throttle"] = 0.35
        if "åœ" in text or "åˆ¹" in text or "åˆ«åŠ¨" in text:
            cmd["command"] = "STOP"; cmd["throttle"] = 0.0; return cmd
        if "å" in text or "é€€" in text: cmd["command"] = "BACKWARD"
        if "å·¦" in text: cmd["steer"] = -1.0
        elif "å³" in text: cmd["steer"] = 1.0
        if "å¿«" in text: cmd["throttle"] = 0.6
        if "æ…¢" in text: cmd["throttle"] = 0.2
        if "ä¸€ç‚¹" in text or "å¾®" in text: 
            cmd["throttle"] = 0.2; 
            if cmd["steer"] != 0: cmd["steer"] *= 0.3 
        return cmd

    return None

def get_llm_command(text: str):
    payload = {
        "model": LLM_MODEL_NAME,
        "messages": [{"role": "system", "content": SYSTEM_PROMPT}, {"role": "user", "content": text}],
        "stream": False, "temperature": 0.1, "response_format": {"type": "json_object"}
    }
    try:
        print(f"ğŸ¤– DeepSeek æ€è€ƒ: '{text}' ...")
        res = session.post(API_URL, json=payload, timeout=3)
        r = res.json()['choices'][0]['message']['content'].strip()
        if "```" in r: r = r.replace("```json", "").replace("```", "")
        return json.loads(r)
    except Exception as e:
        print(f"âŒ LLM Error: {e}")
        return None

def audio_loop():
    global CURRENT_MODE, PROGRAM_RUNNING, CURRENT_TARGET, FORCE_UNLOCK_SIGNAL
    
    print("ğŸ™ï¸ åˆå§‹åŒ–è¯­éŸ³...")
    try:
        asr_model = AutoModel(model=ASR_MODEL_PATH, device="cuda" if torch.cuda.is_available() else "cpu", disable_update=True, log_level="ERROR")
        vad_config = VadModelConfig(SileroVadModelConfig(model=VAD_PATH, min_silence_duration=0.5, threshold=0.5), sample_rate=16000)
        vad = VoiceActivityDetector(vad_config, buffer_size_in_seconds=100)
        print("âœ… è¯­éŸ³å°±ç»ª")
    except Exception as e:
        print(f"âŒ è¯­éŸ³åŠ è½½å¤±è´¥: {e}"); return

    sample_rate = 16000
    samples_per_read = int(0.1 * sample_rate)
    
    with sd.InputStream(channels=1, dtype="float32", samplerate=sample_rate) as s:
        while PROGRAM_RUNNING:
            samples, _ = s.read(samples_per_read)
            samples = samples.reshape(-1)
            vad.accept_waveform(samples)
            
            if not vad.empty():
                audio_segment = np.array(vad.front.samples)
                vad.pop()
                if len(audio_segment) > 0:
                    try:
                        res = asr_model.generate(input=[audio_segment], cache={}, language="zh", use_itn=True, batch_size_s=60)
                        text = res[0].get("text", "") if res else ""
                        text = re.sub(r'<\|.*?\|>', '', text).strip()
                        
                        if len(text) > 0:
                            print(f"\nğŸ‘‚: {text}")
                            cmd_data = parse_local_logic(text)
                            if cmd_data is None: cmd_data = get_llm_command(text)
                            if not cmd_data: continue

                            # 1. å¼ºåˆ¶è§£é”
                            if cmd_data.get("unlock"):
                                FORCE_UNLOCK_SIGNAL = True
                                print("ğŸ”“ æ­£åœ¨æ¥è§¦é”å®š...")
                                continue

                            # 2. åˆ‡æ¢ç›®æ ‡
                            if cmd_data.get("new_target"):
                                CURRENT_TARGET = cmd_data["new_target"]
                                FORCE_UNLOCK_SIGNAL = True # åˆ‡æ¢ç›®æ ‡æ—¶ä¹Ÿå…ˆè§£é”æ—§çš„
                                print(f"ğŸ¯ ç›®æ ‡åˆ‡æ¢ä¸º: {CURRENT_TARGET}")
                            
                            # 3. åˆ‡æ¢æ¨¡å¼
                            if cmd_data.get("mode_switch"):
                                new_mode = cmd_data["mode_switch"]
                                if new_mode != CURRENT_MODE:
                                    CURRENT_MODE = new_mode
                                    print(f"ğŸ”€ æ¨¡å¼: {CURRENT_MODE}")
                                    send_command("STOP", 0.0, 0.0)
                                    continue 
                            
                            if CURRENT_MODE == "VOICE":
                                send_command(cmd_data.get("command", "STOP"), cmd_data.get("steer", 0.0), cmd_data.get("throttle", 0.0))
                                
                    except Exception as e:
                        print(f"Error: {e}")

if __name__ == "__main__":
    t_video = Thread(target=video_loop, daemon=True)
    t_video.start()
    try:
        audio_loop()
    except KeyboardInterrupt: pass
    finally:
        PROGRAM_RUNNING = False
        send_command("STOP", 0.0, 0.0)