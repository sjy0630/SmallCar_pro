import sounddevice as sd
import numpy as np
import requests
import json
import time
import os
import sys
import cv2
import torch
import re
from threading import Thread, Lock
from contextlib import contextmanager

# --- ÂºïÂÖ•ËØ≠Èü≥‰æùËµñ ---
from funasr import AutoModel
from sherpa_onnx import VadModelConfig, SileroVadModelConfig, VoiceActivityDetector

# --- [Êñ∞Â¢û] ÂºïÂÖ• ModelScope Áî®‰∫éÊú¨Âú∞ËØ≠‰πâÂµåÂÖ• ---
from modelscope.pipelines import pipeline
from modelscope.utils.constant import Tasks

# ==========================================
#               ÂÖ®Â±ÄÈÖçÁΩÆÂå∫
# ==========================================

CAR_IP = "172.24.225.17"
CONTROL_URL = f"http://{CAR_IP}:5000/control"
STREAM_URL = f"http://{CAR_IP}:8080/?action=stream"

API_KEY = "sk-8259e96168f94f4baf816ec8769b726a"
API_URL = "https://api.deepseek.com/chat/completions"
LLM_MODEL_NAME = "deepseek-chat"

VAD_PATH = 'voice_models/VAD/silero_vad.onnx' 
ASR_MODEL_PATH = "iic/SenseVoiceSmall" 
# [Êñ∞Â¢û] Êú¨Âú∞ËØ≠‰πâÊ®°Âûã (ÈòøÈáåÂ∑¥Â∑¥ËææÊë©Èô¢Âá∫ÂìÅÔºåËΩªÈáèÈ´òÊïà)
EMBEDDING_MODEL_PATH = "damo/nlp_corom_sentence-embedding_chinese-base"

CN_COCO_MAP = {
    "‰∫∫": "person", "Êàë": "person", "Ëá™Â∑±": "person",
    "Áì∂": "bottle", "Ê∞¥": "bottle", "ÊùØ": "cup",
    "ÊâãÊú∫": "cell phone", "ÁîµËØù": "cell phone",
    "‰π¶": "book", "Áå´": "cat", "Áãó": "dog",
    "ÈîÆÁõò": "keyboard", "Èº†Ê†á": "mouse",
    "Ââ™ÂàÄ": "scissors", "ÈÅ•Êéß": "remote"
}

CURRENT_MODE = "VOICE"
CURRENT_TARGET = "person"
PROGRAM_RUNNING = True
FORCE_UNLOCK_SIGNAL = False
LAST_CMD_STATE = {"command": "STOP", "steer": 0.0, "throttle": 0.0}

video_lock = Lock()
latest_frame = None
latest_ret = False

session = requests.Session()
session.headers.update({"Authorization": f"Bearer {API_KEY}", "Content-Type": "application/json"})

# ==========================================
#           Ê†∏ÂøÉÂ∑•ÂÖ∑ÔºöÈùôÈü≥Âô®
# ==========================================
@contextmanager
def no_print():
    """Â±èËîΩÁªàÁ´ØËæìÂá∫"""
    with open(os.devnull, "w") as devnull:
        old_stdout = sys.stdout; old_stderr = sys.stderr
        try:
            sys.stdout = devnull; sys.stderr = devnull
            yield
        finally:
            sys.stdout = old_stdout; sys.stderr = old_stderr

# ==========================================
#       [Ê†∏ÂøÉÂçáÁ∫ß] Êú¨Âú∞ AI ËØ≠‰πâË∑ØÁî±Âô®
# ==========================================
class SemanticRouter:
    def __init__(self):
        print("üß† Ê≠£Âú®Âä†ËΩΩÊú¨Âú∞ËØ≠‰πâÁêÜËß£Ê®°Âûã (È¶ñÊ¨°ÈúÄ‰∏ãËΩΩ)...")
        try:
            # Âä†ËΩΩ embedding Ê®°Âûã
            self.pipeline = pipeline(Tasks.sentence_embedding, model=EMBEDDING_MODEL_PATH)
            
            # ÂÆö‰πâÊ†áÂáÜÊåá‰ª§Â∫ì (ÈîöÁÇπ)
            # ‰Ω†ÁöÑËØù‰ºöÂíåËøô‰∫õÈîöÁÇπÊØîÂØπÔºåËÄå‰∏çÂè™ÊòØÊü•ÂÖ≥ÈîÆÂ≠ó
            self.command_anchors = {
                "UNLOCK": ["Ëß£Èô§ÈîÅÂÆö", "ÂÅúÊ≠¢ËøΩË∏™", "Âà´ËøΩ‰∫Ü", "‰∏çË¶ÅË∑ü‰∫Ü", "ÂèñÊ∂à", "ÊùæÊâã"],
                "TRACK_MODE": ["ÂºÄÂêØËøΩË∏™", "ÂàáÊç¢Âà∞ËøΩË∏™Ê®°Âºè", "Ë∑üÊàëËµ∞", "Ëá™Âä®Ë∑üÈöè", "ÂºÄÂßãÊâæ‰∫∫"],
                "VOICE_MODE": ["ÂàáÊç¢Âà∞ËØ≠Èü≥Ê®°Âºè", "ÊâãÂä®ÊéßÂà∂", "Âê¨ÊàëÊåáÊå•", "Êîπ‰∏∫ËØ≠Ë®ÄÊéßÂà∂", "ÂõûÊù•"],
                "FORWARD": ["ÂâçËøõ", "ÂæÄÂâçËµ∞", "ÂêëÂâç", "Ëµ∞Ëµ∑Êù•", "Go"],
                "BACKWARD": ["ÂêéÈÄÄ", "ÂÄíËΩ¶", "ÂæÄÂêéÈÄÄ", "ÈÄÄÂõûÊù•"],
                "LEFT": ["Â∑¶ËΩ¨", "ÂæÄÂ∑¶Êãê", "ÂêëÂ∑¶"],
                "RIGHT": ["Âè≥ËΩ¨", "ÂæÄÂè≥Êãê", "ÂêëÂè≥"],
                "STOP": ["ÂÅúÊ≠¢", "ÂÅú‰∏ã", "Âà´Âä®", "ÂàπËΩ¶", "Á´ãÂÆö"],
                "SPEED_UP": ["Â§™ÊÖ¢‰∫Ü", "Âä†ÈÄü", "Âø´ÁÇπË∑ë", "ÊèêÈÄü"],
                "SLOW_DOWN": ["Â§™Âø´‰∫Ü", "ÂáèÈÄü", "ÊÖ¢‰∏ÄÁÇπ", "ÊÖ¢ÁÇπË∑ë"]
            }
            
            # È¢ÑËÆ°ÁÆóÈîöÁÇπÁöÑÂêëÈáè
            self.anchor_embeddings = {}
            for intent, texts in self.command_anchors.items():
                # ËÆ°ÁÆó‰∏ÄÁªÑÈîöÁÇπÁöÑÂπ≥ÂùáÂêëÈáè
                embeddings = self.get_embeddings(texts)
                # ÁÆÄÂçïÂπ≥Âùá‰Ωú‰∏∫ËØ•ÊÑèÂõæÁöÑ‰∏≠ÂøÉÂêëÈáè
                self.anchor_embeddings[intent] = np.mean(embeddings, axis=0)
                
            print("‚úÖ ËØ≠‰πâÂ§ßËÑëÊûÑÂª∫ÂÆåÊàêÔºÅ")
            self.ready = True
        except Exception as e:
            print(f"‚ùå ËØ≠‰πâÊ®°ÂûãÂä†ËΩΩÂ§±Ë¥•: {e}")
            self.ready = False

    def get_embeddings(self, texts):
        if isinstance(texts, str): texts = [texts]
        # modelscope ÁöÑ pipeline ËæìÂÖ•Ê†ºÂºè
        inputs = {'source_sentence': texts}
        result = self.pipeline(input=inputs)
        return np.array([x['embedding'] for x in result['output']])

    def predict(self, text, threshold=0.60):
        """
        ËæìÂÖ•ÊñáÊú¨ÔºåËøîÂõû (Intent, Score)
        Â¶ÇÊûúÊúÄÈ´òÂàÜ‰Ωé‰∫é thresholdÔºåËØ¥ÊòéÊòØÂ§çÊùÇÊåá‰ª§ÔºåËøîÂõû None
        """
        if not self.ready: return None, 0.0
        
        # 1. ËÆ°ÁÆóËæìÂÖ•ÊñáÊú¨ÁöÑÂêëÈáè
        input_emb = self.get_embeddings(text)[0]
        
        # 2. ËÆ°ÁÆó‰∏éÊâÄÊúâÊÑèÂõæÁöÑ‰ΩôÂº¶Áõ∏‰ººÂ∫¶
        best_intent = None
        best_score = -1.0
        
        for intent, anchor_emb in self.anchor_embeddings.items():
            # Cosine Similarity
            score = np.dot(input_emb, anchor_emb) / (np.linalg.norm(input_emb) * np.linalg.norm(anchor_emb))
            if score > best_score:
                best_score = score
                best_intent = intent
        
        if best_score >= threshold:
            return best_intent, best_score
        else:
            return None, best_score

# ÂàùÂßãÂåñË∑ØÁî±Âô®
router = SemanticRouter()

# ==========================================
#           Ê®°Âùó 1: ËßÜËßâËøΩË∏™ (v9ÁâàÈÄªËæë)
# ==========================================
# (‰∏∫‰∫ÜËäÇÁúÅÁØáÂπÖÔºåËøôÈáå‰øùÊåÅ v9 ÁöÑ YOLO ‰ª£Á†ÅÈÄªËæë‰∏çÂèòÔºåÂäüËÉΩÂÆåÂÖ®‰∏ÄËá¥)
Kp = 0.35; Ki = 0.0; Kd = 1.0; CENTER_DEAD_ZONE = 0.12; STOP_AREA_THRESHOLD = 0.35; MAX_LOCK_AREA = 0.7; CONF_THRESHOLD = 0.35; ERROR_BUFFER_SIZE = 3
integral = 0.0; previous_error = 0.0; last_pid_time = time.time(); error_buffer = []

def load_yolo_model():
    print("üì∑ Ê≠£Âú®Âä†ËΩΩÊú¨Âú∞ YOLOv5s Ê®°Âûã (Á¶ªÁ∫øÊ®°Âºè)...")
    try:
        model = torch.hub.load('./yolov5', 'custom', path='yolov5s.pt', source='local')
        model.conf = CONF_THRESHOLD; model.iou = 0.45
        if torch.cuda.is_available(): model.cuda()
        print("‚úÖ YOLOv5s Âä†ËΩΩÊàêÂäüÔºÅ")
        return model
    except Exception as e:
        print(f"‚ùå ËßÜËßâÊ®°ÂûãÂä†ËΩΩÂ§±Ë¥•: {e}"); return None

def get_pid_command(box, h, w):
    global integral, previous_error, last_pid_time, error_buffer
    x_center = (box[0] + box[2]) / 2 / w
    box_area = (box[2] - box[0]) * (box[3] - box[1]) / (h * w)
    if box_area > STOP_AREA_THRESHOLD: integral=0.0; previous_error=0.0; error_buffer=[]; return "STOP", 0.0
    curr_time = time.time(); dt = curr_time - last_pid_time; 
    if dt == 0: dt = 1e-5
    error_buffer.append(x_center - 0.5)
    if len(error_buffer) > ERROR_BUFFER_SIZE: error_buffer.pop(0)
    smooth_error = sum(error_buffer) / len(error_buffer)
    if abs(smooth_error) < CENTER_DEAD_ZONE: smooth_error = 0.0; previous_error = 0.0
    integral += smooth_error * dt; integral = max(-1.0, min(1.0, integral))
    derivative = (smooth_error - previous_error) / dt
    active_Kp = 0.55 if CURRENT_TARGET != "person" else 0.35
    steer = (active_Kp * smooth_error) + (Ki * integral) + (Kd * derivative)
    previous_error = smooth_error; last_pid_time = curr_time
    return "FORWARD", max(-1.0, min(1.0, steer))

def video_loop():
    global latest_frame, latest_ret, CURRENT_MODE, PROGRAM_RUNNING, CURRENT_TARGET, FORCE_UNLOCK_SIGNAL
    yolo = load_yolo_model()
    if not yolo: return
    cap = cv2.VideoCapture(STREAM_URL)
    def read_stream():
        global latest_frame, latest_ret
        while PROGRAM_RUNNING:
            try:
                ret, frame = cap.read()
                with video_lock: latest_frame = frame; latest_ret = ret
                if not ret: time.sleep(0.5)
                else: time.sleep(0.01)
            except: pass
    Thread(target=read_stream, daemon=True).start()
    tracker = None; is_tracking = False; frames_since = 0
    print("‚úÖ ËßÜËßâÁ∫øÁ®ãÂêØÂä® (v10 AIÁâà)")
    while PROGRAM_RUNNING:
        try:
            with video_lock: frame = latest_frame.copy() if latest_frame is not None else None
            if frame is None: time.sleep(0.1); continue
            h, w, _ = frame.shape
            if FORCE_UNLOCK_SIGNAL: is_tracking = False; tracker = None; FORCE_UNLOCK_SIGNAL = False; send_command("STOP", 0, 0); print("üîì ËßÜËßâÂ∑≤ÈáçÁΩÆ")
            if CURRENT_MODE == "TRACK":
                need_detect = (not is_tracking) or (frames_since > 10)
                cands = []
                if need_detect:
                    frames_since = 0
                    try:
                        results = yolo(frame)
                        for det in results.xyxy[0].cpu().numpy():
                            x1, y1, x2, y2, conf, cls = det
                            if int(cls) < len(results.names):
                                name = results.names[int(cls)]
                                if name == CURRENT_TARGET:
                                    cv2.rectangle(frame, (int(x1),int(y1)), (int(x2),int(y2)), (255,0,0), 1)
                                    cv2.putText(frame, f"{name} {conf:.2f}", (int(x1), int(y1)-5), 0, 0.5, (255,0,0), 1)
                                    b_w = x2-x1; b_h = y2-y1; ratio = (b_w*b_h)/(w*h)
                                    if CURRENT_TARGET == "person":
                                        if ratio < MAX_LOCK_AREA and b_h > (b_w * 0.7): cands.append([x1,y1,x2,y2])
                                    else:
                                        if ratio < 0.9: cands.append([x1,y1,x2,y2])
                    except: pass
                if not is_tracking:
                    if cands:
                        best = min(cands, key=lambda b: ((b[0]+b[2])/2/w-0.5)**2 + ((b[1]+b[3])/2/h-0.5)**2)
                        tracker = cv2.TrackerCSRT_create()
                        tracker.init(frame, (int(best[0]), int(best[1]), int(best[2]-best[0]), int(best[3]-best[1])))
                        is_tracking = True; print(f"üéØ ËßÜËßâÈîÅÂÆö: {CURRENT_TARGET}")
                    else: cv2.putText(frame, f"SEARCHING: {CURRENT_TARGET}...", (20, 80), 0, 0.7, (0,255,255), 2)
                else:
                    frames_since += 1
                    ok, bbox = tracker.update(frame)
                    if ok:
                        p1 = (int(bbox[0]), int(bbox[1])); p2 = (int(bbox[0]+bbox[2]), int(bbox[1]+bbox[3]))
                        cv2.rectangle(frame, p1, p2, (0,255,0), 3)
                        cv2.putText(frame, "LOCKED", (p1[0], p1[1]-10), 0, 0.7, (0,255,0), 2)
                        cmd, steer = get_pid_command([bbox[0], bbox[1], bbox[0]+bbox[2], bbox[1]+bbox[3]], h, w)
                        send_command(cmd, steer, 0.35 if cmd=="FORWARD" else 0.0)
                    else: is_tracking = False; tracker = None; send_command("STOP", 0, 0)
            else:
                if is_tracking: is_tracking=False; tracker=None
                cv2.putText(frame, "VOICE MODE", (20, 40), 0, 1.0, (0,0,255), 2)
            cv2.imshow('SmartCar Vision v10', frame)
            if cv2.waitKey(1) == ord('q'): PROGRAM_RUNNING=False; break
        except Exception as e: time.sleep(1)
    cap.release(); cv2.destroyAllWindows()

# ==========================================
#           Ê®°Âùó 2: ËØ≠Èü≥ÊéßÂà∂ (ËØ≠‰πâË∑ØÁî± + LLM)
# ==========================================

SYSTEM_PROMPT = """
‰Ω†ÊòØ‰∏Ä‰∏™Êô∫ËÉΩÂ∞èËΩ¶ÁöÑÊéßÂà∂Â§ßËÑë„ÄÇËØ∑Â∞ÜÁî®Êà∑ÁöÑÂè£ËØ≠Êåá‰ª§ËΩ¨Êç¢‰∏∫ JSON ÊéßÂà∂‰ø°Âè∑„ÄÇ
{ "command": "FORWARD/BACKWARD/STOP", "steer": -1.0Âà∞1.0, "throttle": 0.0Âà∞1.0, "target": "person/cup/bottle/..." }
"""

def send_command(cmd, steer, throttle):
    global LAST_CMD_STATE
    LAST_CMD_STATE = {"command": cmd, "steer": steer, "throttle": throttle}
    try: requests.post(CONTROL_URL, json=LAST_CMD_STATE, timeout=0.2)
    except: pass 

def handle_ai_command(text):
    """
    ËØ≠‰πâÂ§ÑÁêÜÊ†∏ÂøÉÈÄªËæë
    """
    cmd_data = {"mode_switch": None, "command": "STOP", "steer": 0.0, "throttle": 0.0, "new_target": None, "unlock": False}
    
    # --- 1. ‰ΩøÁî®Êú¨Âú∞ AI Ê®°ÂûãÂà§Êñ≠ÊÑèÂõæ ---
    intent, score = router.predict(text)
    
    # ÊâìÂç∞ÂåπÈÖçÁªìÊûú‰æõË∞ÉËØï
    if intent:
        print(f"üß† Êú¨Âú∞AIÂà§Êñ≠: [{intent}] (ÁΩÆ‰ø°Â∫¶: {score:.2f})")
    
    # Â¶ÇÊûúÁΩÆ‰ø°Â∫¶È´òÔºåÁõ¥Êé•ÊâßË°åÊú¨Âú∞È¢ÑËÆæÈÄªËæë
    if intent and score > 0.65:
        # Ê®°Âºè‰∏éËß£ÈîÅ
        if intent == "UNLOCK": cmd_data["unlock"] = True; return cmd_data
        if intent == "TRACK_MODE": cmd_data["mode_switch"] = "TRACK"; return cmd_data
        if intent == "VOICE_MODE": cmd_data["mode_switch"] = "VOICE"; return cmd_data
        
        # ËøêÂä®ÊéßÂà∂
        current_steer = 0.0
        current_throttle = 0.35
        current_cmd = "FORWARD"
        
        # ÁªßÊâøÊóßÁä∂ÊÄÅ
        if intent in ["SPEED_UP", "SLOW_DOWN"]:
            current_cmd = LAST_CMD_STATE.get("command", "FORWARD")
            if current_cmd == "STOP": current_cmd = "FORWARD"
            current_steer = LAST_CMD_STATE.get("steer", 0.0)
            
        if intent == "FORWARD": current_cmd = "FORWARD"
        elif intent == "BACKWARD": current_cmd = "BACKWARD"
        elif intent == "STOP": current_cmd = "STOP"
        
        if intent == "LEFT": current_steer = -1.0; current_cmd = "FORWARD"
        elif intent == "RIGHT": current_steer = 1.0; current_cmd = "FORWARD"
        
        if intent == "SPEED_UP": current_throttle = 0.6
        elif intent == "SLOW_DOWN": current_throttle = 0.2
            
        cmd_data["command"] = current_cmd
        cmd_data["steer"] = current_steer
        cmd_data["throttle"] = current_throttle
        return cmd_data
    
    # --- 2. ÁõÆÊ†áÊ£ÄÊµãÂàáÊç¢ (‰ªç‰øùÁïôÁÆÄÂçïÁöÑÂÖ≥ÈîÆËØçÔºåÂõ†‰∏∫Áâ©‰ΩìÂêçÂ≠óÂ§™Â§ö) ---
    tgt = next((v for k,v in CN_COCO_MAP.items() if k in text), None)
    if ("ËøΩ" in text or "Ë∑ü" in text or "Êâæ" in text) and tgt:
        cmd_data["mode_switch"]="TRACK"
        cmd_data["new_target"]=tgt
        return cmd_data

    # --- 3. Â¶ÇÊûúÊú¨Âú∞ AI ËßâÂæó‰∏çÂÉè‰ªª‰ΩïÊ†áÂáÜÊåá‰ª§Ôºå‰∫§Áªô LLM ---
    print("ü§î Êú¨Âú∞Ê®°ÂûãÊ≤°Âê¨ÊáÇÔºåËØ∑Ê±Ç LLM ÊîØÊè¥...")
    return None # ËøîÂõû None Ëß¶ÂèëÂ§ñÈÉ® LLM Ë∞ÉÁî®

def audio_loop():
    global CURRENT_MODE, PROGRAM_RUNNING, CURRENT_TARGET, FORCE_UNLOCK_SIGNAL
    print("üéôÔ∏è ÂàùÂßãÂåñËØ≠Èü≥ (v10 AIËØ≠‰πâÂ¢ûÂº∫Áâà)...")
    try:
        asr = AutoModel(model=ASR_MODEL_PATH, disable_update=True, log_level="ERROR")
        vad = VoiceActivityDetector(VadModelConfig(SileroVadModelConfig(model=VAD_PATH, min_silence_duration=0.5, threshold=0.5), sample_rate=16000), buffer_size_in_seconds=100)
        print("‚úÖ ËØ≠Èü≥Â∞±Áª™")
    except Exception as e: print(f"‚ùå ËØ≠Èü≥ÊåÇ‰∫Ü: {e}"); return

    sr = 16000; batch = int(0.1*sr)
    with sd.InputStream(channels=1, dtype="float32", samplerate=sr) as s:
        while PROGRAM_RUNNING:
            d, _ = s.read(batch); d = d.reshape(-1)
            vad.accept_waveform(d)
            if not vad.empty():
                raw = np.array(vad.front.samples); vad.pop()
                if len(raw) > 0:
                    try:
                        with no_print(): 
                            res = asr.generate(input=[raw], cache={}, language="zh", use_itn=True, batch_size_s=60)
                        
                        text = res[0].get("text", "") if res else ""; text = re.sub(r'<\|.*?\|>', '', text).strip()
                        if text:
                            print(f"\nüëÇ: {text}")
                            
                            # Ë∞ÉÁî® AI Ë∑ØÁî±ÈÄªËæë
                            data = handle_ai_command(text)
                            
                            # Â¶ÇÊûúÊú¨Âú∞ AI Êêû‰∏çÂÆöÔºåÂëºÂè´ DeepSeek
                            if not data and len(text) > 1: 
                                payload = {"model": LLM_MODEL_NAME, "messages": [{"role": "system", "content": SYSTEM_PROMPT}, {"role": "user", "content": text}], "stream": False, "response_format": {"type": "json_object"}}
                                try: 
                                    r = session.post(API_URL, json=payload, timeout=3).json()['choices'][0]['message']['content']
                                    data = json.loads(r.replace("```json","").replace("```",""))
                                    print(f"ü§ñ LLM ÂìçÂ∫î: {data}")
                                except: pass

                            if not data: continue
                            
                            if data.get("unlock"): FORCE_UNLOCK_SIGNAL=True; print("üîì Êåá‰ª§: Ëß£Èô§ËøΩË∏™"); continue
                            if data.get("new_target"): CURRENT_TARGET=data["new_target"]; FORCE_UNLOCK_SIGNAL=True; print(f"üéØ Êñ∞ÁõÆÊ†á: {CURRENT_TARGET}")
                            if data.get("mode_switch"): 
                                if data["mode_switch"] != CURRENT_MODE: CURRENT_MODE=data["mode_switch"]; print(f"üîÄ Ê®°Âºè: {CURRENT_MODE}"); send_command("STOP",0,0); continue
                            
                            if CURRENT_MODE=="VOICE": 
                                send_command(data.get("command","STOP"), data.get("steer",0), data.get("throttle",0))
                    except Exception as e: 
                        if "WinError 6" not in str(e): print(f"ËØ≠Èü≥ÈîôËØØ: {e}")

if __name__ == "__main__":
    t = Thread(target=video_loop, daemon=True); t.start()
    try: audio_loop()
    except KeyboardInterrupt: pass
    finally: PROGRAM_RUNNING=False; send_command("STOP",0,0)